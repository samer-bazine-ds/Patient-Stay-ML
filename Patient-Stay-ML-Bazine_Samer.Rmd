---
title: "ML TEST SAMER BAZINE "
author: "samer"
date: "2025-05-04"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
``` 
 
 ####################                                  
  #######
 ####################
 
 

'''''''INTODUCTION'''''''


The recent Covid-19 pandemic has underscored the critical importance of efficient healthcare management, particularly in 
optimizing hospital resource allocation to enhance patient outcomes and operational effectiveness. One key parameter in this 
domain is the patient Length of Stay (LOS), which represents the duration a patient remains admitted in a hospital. Accurate 
prediction of LOS at the time of admission enables hospitals to identify patients at risk of prolonged stays, allowing for 
tailored treatment plans to minimize LOS, reduce the risk of infections, and streamline logistics such as bed and room 
allocation. This project, undertaken by a non-profit organization dedicated to improving hospital operations, aims to develop a 
robust predictive model to forecast LOS for each patient on a case-by-case basis. The dataset provided, "LOS.csv," contains 
various patient and hospital-related features, with LOS categorized into 11 classes ranging from 0-10 days to over 100 days. The 
primary objective is to leverage data science techniques to build and evaluate multiple predictive models, ensuring accurate LOS 
classification to support optimal hospital functioning and resource management. Through comprehensive data exploration, model 
development, and performance evaluation, this project seeks to deliver actionable insights for healthcare management while
adhering to the principles of precision and efficiency.

```{r}
library(tidymodels)
library(tidyverse)
library(finetune)
library(kknn)
library(kernlab)
library(ranger)
library(lightgbm)
library(bonsai)
library(themis)
library(ggplot2)
library(baguette)
library(dials)
library(pROC)
```
 ####################  
  #######
  ####################

@@ Q1  read the data with simple discribe

```{r}
df=read.csv(file.choose())
glimpse(df)

```


know, we need to make some variable as character to make a good analysis 



```{r}
df<-df%>%mutate(Hospital_code=as.character(Hospital_code),City_Code_Hospital=as.character(City_Code_Hospital)
                ,Hospital_type_code=as.character(Hospital_type_code))
```
 ####################  
  #######
  ####################

@@ Q2
```{r}
cat("Summary Statistics for Numerical Variables:")
summary(df %>% select_if(is.numeric))
cat("Summary Statistics for Categorical Variables:")
summary(df %>% select_if(is.character))

```
 ####################  
  #######
  ####################

@@ Q2:  some descriptive analysis 

```{r}

cat("Target Variable (Stay) Distribution")
stay_dist <- prop.table(table(df$Stay)) %>% 
  as.data.frame() %>% 
  arrange(Var1)
print(stay_dist)
ggplot(df, aes(x = Stay)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Distribution of Length of Stay", x = "Length of Stay", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
The distribution of hospital stays is positively skewed, indicating that shorter stays are more common. The most frequent stay duration falls within the 21–30 day range, accounting for approximately 8000 cases. The majority of stays are concentrated between 0 and 60 days, with a noticeable decline after 50 days. Stays exceeding 100 days are rare.
and we that our data is umbalanced so we must handl it





```{r}


numerical_cols <- c("Available.Extra.Rooms.in.Hospital", "Visitors.with.Patient", 
                    "Admission_Deposit", "Bed.Grade")

cat("\n=== Numerical Variables vs Stay ===\n\n")


cat("=== Combined Boxplots for All Numerical Variables ===\n")

df_long <- df %>%
  select(all_of(numerical_cols), Stay) %>%
  pivot_longer(cols = all_of(numerical_cols), names_to = "Variable", values_to = "Value")

p_combined <- ggplot(df_long, aes(x = Stay, y = Value)) +
  geom_boxplot(fill = "lightblue", color = "red", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free_y", ncol = 2) +
  labs(title = "Numerical Variables vs Length of Stay",
       x = "Length of Stay", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        plot.title = element_text(hjust = 0.5, face = "bold"),
        axis.title = element_text(face = "bold"),
        strip.text = element_text(face = "bold"))
print(p_combined)

```
Admission deposit 

The median admission deposit remains stable (4500–5000) across all stay durations, with most values between 3000 and 6000. High outliers >9000 are more common in shorter stays, suggesting occasional costly admissions. Overall, admission deposit shows little variation with stay length, indicating it may not be a strong direct predictor.

Available Extra Rooms 
The median number of extra rooms remains steady (3–5) across all stay durations, with some mild variability. Outliers (12 rooms) appear in both short and long stays but in the real may be not an outlier because may be 12 room availabels. Overall, no clear relationship with stay length is observed, though room availability may still indirectly impact hospital operations or discharge timing.

Bed grade
remains consistent across all stay durations, with a median of 2–3 and minimal variation. Outliers (grades 1 and 4) appear evenly across bins, showing no strong pattern. but in real its not a outliers it is just a good gread (4)


Visitors with Patient
Most stays have a low median visitor count (2–3), but longer stays (especially 91–100 days) show higher medians and wider spread, with outliers reaching up to 30. Visitor count increases slightly with stay length, possibly reflecting more serious conditions. 



```{r}
categorical_cols <- c("Department", "Ward_Type","Severity.of.Illness","Type.of.Admission", "Age")

cat("Categorical Variables vs Stay")
for (col in categorical_cols) {
  p <- ggplot(df, aes_string(x = col,fill="Stay")) +
    geom_bar(position = "fill") +
    labs(title = paste(col, "vs Length of Stay"),x=col,y="Proportion") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), 
          legend.position = "right")
  print(p)
}
```
Department :

Shorter stays (11–30 days) dominate across all departments, confirming earlier findings.
Very short stays (0–10 days) are the least common across the board.
The 21–30 day bin consistently shows the highest proportion of stays.
Most departments are dominated by short- to medium-length stays.
Radiotherapy and TB & Chest Disease stand out for having more long-duration cases, suggesting their patients may need more extended care.
This distribution highlights potential resource and care planning differences across departments


Ward Type :
The chart compares stay duration distributions across six ward types (P, Q, R, S, T, U)
Most wards show a high proportion of stays between 11–30 days, reflecting the dataset peak in that range
Wards P, Q, R: Associated with shorter stays, likely routine or procedural care.
Wards S, T: Show mixed durations possibly for more complex cases.
Ward U: Heavily skewed toward extended stays, indicating specialized or long-term patient care.


Severity of Illness:
11–30 day stays dominate across all severity levels, matching the overall dataset trend.
Short stays (0–10 days) are relatively rare for all groups.
There’s a clear positive correlation between illness severity and hospital stay duration:
Extreme cases → tend to stay longer.
Moderate cases → middle ground.
Minor cases → recover faster, shorter stays.
This makes Severity_of_Illness a strong candidate for predicting length of stay.


Type of Admission :
11–30 day stays dominate across all admission types, aligning with the overall length-of-stay distribution.
Short stays (0–10 days) are relatively rare in all categories.
While Emergency and Trauma admissions show a wider spread into longer stays, Urgent cases tend to resolve more quickly.
Overall, Type_of_Admission has moderate predictive value for length of stay—less impactful than severity or ward type but still relevant.


Age :
Across all age groups, 11–30 day stays are the most common.
Short stays (0–10 days) are relatively uncommon throughout.
Age correlates modestly with longer hospital stays—the older the patient, the more likely extended stays become.
Age is a meaningful predictor of length of stay, especially when differentiating between younger and elderly patients.
Older individuals (especially 70+) are significantly more likely to experience prolonged hospitalizations.





```{r}
numeric_data <- df %>% 
  select_if(is.numeric)

cor_matrix <- cor(numeric_data, use = "complete.obs")

ggplot(as.data.frame(as.table(cor_matrix)), aes(x = Var1, y = Var2, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = round(Freq, 2)), color = "black") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "lightblue", midpoint = 0) +
  labs(title = "Correlation Matrix of Numerical Variables", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
No strong correlations are present
All correlations are weak (between -0.15 and 0.23), suggesting low multicollinearity.
This independence allows all variables to be safely included in modeling without redundancy concerns.
However, because the relationships are so weak, these features may only modestly influence predictions or have limited interaction effects on the target  stay



missing_values :

```{r}

missing_values <- colSums(is.na(df))
print("Missing values per column:")
print(missing_values[missing_values > 0])
```
so we must handl it


```{r}
cat("Unique Values in Identifier Columns")
cat("Unique case_id:", length(unique(df$case_id)))
cat("Unique patientid:", length(unique(df$patientid)))
cat("Unique Hospital_code:", length(unique(df$Hospital_code)))
```
The dataset contains 31,842 unique case_id values, indicating each row represents a distinct case
 There are 26,289 unique patientid values, suggesting some patients have multiple cases 
 The 32 unique Hospital cod values indicate the data spans 32 distinct hospitals

```{r}

df<-df%>%select(-c(Department,Ward_Facility_Code,X,case_id))


```



and let's make the target as a factor
and see some summary

```{r}
df<-df%>%mutate(Stay=as.factor(Stay))
summary(df)

```
 ####################  
  #######
  ####################

@@Q3 AND Q4




Splits data into 2/3 training and 1/3 testing sets, imputes missing Bed Grade and City Code_Patient with medians, normalizes numeric predictors  removes near-zero variance predictors, pools rare nominal categories (threshold = 0.2), creates dummy variables for nominal predictors, and applies SMOTE to oversample minority Stay classes (over_ratio = 0.3, 150 neighbors). The prepped recipe produces a processed training dataset for modeling.
we dont use step_corr because we see that No strong correlations are present in the hetmap 

```{r}
set.seed(2025)
df_split<-df%>%initial_split(2/3)

df_train<-df_split%>%training()
df_test<-df_split%>%testing()
df_rec<-df_train%>%recipe(Stay~. )%>%
  step_impute_median(Bed.Grade) %>%
  step_impute_median(City_Code_Patient) %>%
  step_normalize(all_numeric_predictors())%>%
  step_nzv(all_predictors())%>%
  step_other(all_nominal_predictors(),threshold = 0.2)%>%
  step_dummy(all_nominal_predictors()) %>% 
  step_smote( Stay,over_ratio = 0.3, neighbors = 150 )



```
 justify the use of every step function:

step_impute_median(Bed_Grade) %>%
  step_impute_mode(City_Code_Patient)
because i have a missing values in only this 2 variable 

step_normalize(all_numeric_predictors()):
there is a lot of dispertion so it should be used 
make all numeric features into a comparable scale, improving convergence and interpretability.

step_nzv(all_predictors()):
to remove the feature whiche have near 0 of variance 

step_other(all_nominal_predictors(),threshold = 0.2):
Groups infrequent factor levels (with proportion < 0.2) into a new "other" category. to make the model learn more patterne

step_dummy(all_nominal_predictors()):
to do the dummy  envodiing , it is one of the best methode to do feature engenering and dummy encoding allows the model to treat each category as a separate binary feature.

step_smote( Stay,over_ratio = 0.3, neighbors = 150 ):
 as we se in the distrubition of stay (target variable) there is imbalanced distribution so we need to use the smoote step to handl umbalanced 

we dont use step_corr because we see that No strong correlations are present in the hetmap 




 ####################  
  #######
  ####################


@@ Q5
THE MODELS CAN WE USED FOR THIS PROBLEM IS :
RANDOM FOREST , KNN, SVM , XGBOOST,LIGHTGBM,MULTINOMIAL REGRESSION ,BAGGED DECISION TRESS, DECISION TREES 



start with random forest 
using 340 tres (high number ) to make the performance more good 
mtry automatic ( so it using sqrt of number of variables)

 ####################  
  #######
  ####################

@@q6

```{r}
random_mod <- rand_forest(trees =340) %>%
  set_mode('classification') %>%
  set_engine('ranger')


work_random <- workflow() %>%
  add_model(random_mod) %>%
  add_recipe(df_rec)

random_fit <- work_random %>%
  last_fit(df_split)
```


 ####################  
  #######
  ####################


@Q7 :JUSTIFICATION OF PARAMETRES


MULTINOMIAL regression 

penalty parameter  controls the strength of regularization in the model smaller penalty value means less regularization, allowing the model to fit the training data more closely by assigning larger coefficients to the predictors.
The mixture parameter  controls the balance between L1 (lasso) and L2 (ridge) regularization:
mixture = 0: Pure ridge regression 
mixture = 1: Pure lasso regression
mixture = 0.01: A value very close to 0, meaning the regularization is almost entirely L2 (ridge) with a tiny amount of L1 (lasso).

```{r}
log_mod<-multinom_reg(penalty = 0.001,mixture =0.01) %>%
  set_engine("nnet") %>%  
  set_mode("classification")

work_log<-workflow()%>%
  add_model(log_mod)%>%
  add_recipe(df_rec)

log_fit<-work_log%>%
  last_fit(df_split)
  
```


lightgbm :

using 500 tres (high number ) to make the performance more good 
tree depth 5 to avoid overfiting (dont use high number)
min n introduces randomness, which can reduce overfitting
learning rate to controle the speed of training (0.1) is good 
mtry 1 means no feature subsampling is performed at the tree level
sample_size of 1 means no subsampling is performed
min n = 3 is relatively small, allowing the model to create fine-grained splits


```{r}

lgb_mod<-boost_tree(trees = 500, tree_depth = 5, min_n = 3,learn_rate = 0.1, sample_size = 1, mtry = 1)%>%
 set_mode("classification") %>%
  set_engine("lightgbm")

work_lgb<-workflow()%>%
  add_model(lgb_mod)%>%
  add_recipe(df_rec)

lgb_fit<-work_lgb%>%
  last_fit(df_split)
  
```






DECISIONTRES 
tree_depth controls the maximum depth of the tree. A deeper tree can capture more complex patterns by allowing more splits
 min n 20 is a moderate choice, providing a balance between allowing the tree to grow and preventing overfitting
 cost_complexity of 0.0004 is very small, meaning the tree is pruned minimally, allowing it to grow large and capture detailed patterns in the data

```{r}
tree_mode<-decision_tree( tree_depth = 30,min_n = 20,cost_complexity = 0.0004
) %>%
  set_engine("rpart") %>%    
  set_mode("classification")


work_tree<-workflow()%>%
  add_model(tree_mode)%>%
  add_recipe(df_rec)

tree_fit<-work_tree%>%
  last_fit(df_split)
  
```



xgboost model 
using 600 tres (high number ) to make the performance more good 
tree depth 6 to avoid overfiting (dont use high number)
min n introduces randomness, which can reduce overfitting
learning rate to controle the speed of training (0.1) is good 
mtry 1 means no feature subsampling is performed at the tree level
sample_size of 0.9 means no subsampling is performed

```{r}
xgb_mod<-boost_tree(trees = 600,tree_depth = 6,min_n = 50,learn_rate = 0.1,sample_size = 0.9,mtry = 1
) %>%
  set_mode('classification') %>%
  set_engine('xgboost',)

work_xgb<-workflow()%>%
  add_model(xgb_mod)%>%
  add_recipe(df_rec)

xgb_fit<-work_xgb%>%
  last_fit(df_split)
  
```



KNN
neighbors 120 is relatively large, meaning the model will consider a large number of neighbors when making predictions.
dist_power 1 determines the power parameter for the Minkowski distance metric used to calculate distances between points
```{r}
knn_mod<-nearest_neighbor(neighbors =120 , dist_power =1)%>%
 set_mode('classification')%>%
 set_engine('kknn')
work_knn<-workflow()%>%
  add_model(knn_mod)%>%
  add_recipe(df_rec)

knn_fit<-work_knn%>%
 last_fit(df_split)
  
```
bag_model 

cost_complexity 0.0001 allows for relatively complex trees while still applying light pruning
tree_depth 10 strikes a balance between capturing complex relationships and preventing overfitting
min n ensures that splits occur only when there are enough data points to make reliable decisions
```{r}
bag_mod <- bag_tree(
  cost_complexity = 0.0001,  
  tree_depth = 10,        
  min_n = 5               
) %>%
  set_mode("classification") %>%
  set_engine("rpart", times = 25)  


bag_workflow <- workflow() %>%
  add_model(bag_mod) %>%
  add_recipe(df_rec)  


bag_fit <- bag_workflow %>%
  last_fit(df_split)

```








this is a second recipe used (just change the values of over ratio and neighboors)
```{r}
df_rec1<-df_train%>%recipe(Stay~. )%>%
  step_impute_median(Bed.Grade) %>%
  step_impute_median(City_Code_Patient) %>%
  step_normalize(all_numeric_predictors())%>%
  step_nzv(all_predictors())%>%
  step_other(all_nominal_predictors(),threshold = 0.2)%>%
  step_dummy(all_nominal_predictors()) %>% 
  step_smote( Stay,over_ratio = 1, neighbors = 10 )
```

SVM
SVM model with a rbf kernel
cost  1 is relatively small encouraging a wider margin and a simpler decision boundary.
rbf_sigma = 0.2  2 is a moderate choice


```{r}

svm_mod<-svm_rbf(cost =1 ,rbf_sigma = 0.2 )%>%
  set_mode('classification')%>%
  set_engine('kernlab')
 
work_svm<-workflow()%>%
  add_model(svm_mod)%>%
  add_recipe(df_rec1)
 
svm_fit<-work_svm%>%
  last_fit(df_split)
  
```


 ####################  
  #######
  ####################

@@ q8
and let,s see the results

THE METRIC USED F1 scores
The F1 score treats all classes equally, balancing precision and recall to evaluate model performance, making it valuable for imbalanced datasets where rare classes are critical. Unlike accuracy, which can be skewed by frequent classes, or ROC-AUC, which focuses on overall ranking, the F1 score emphasizes correct predictions for each class individually. It is calculated as the harmonic mean of precision and recall, providing a single metric that penalizes models ignoring minority classes, ensuring robust performance across all classes, especially when rare classes are prioritized

random model

```{r}
random_fit %>% collect_metrics()
result_random<-random_fit %>% augment()

result_random %>% f_meas(truth = Stay,estimate = .pred_class)

```


```{r}
log_fit%>%collect_metrics()

result_log<-log_fit %>% augment()
result_log%>%f_meas(truth = Stay,estimate = .pred_class)
```



```{r}
lgb_fit %>% collect_metrics()

result_lgb<-lgb_fit %>% augment()
result_lgb%>%f_meas(truth = Stay,estimate = .pred_class)
```




```{r}
tree_fit%>%collect_metrics()

result_trees<-tree_fit %>% augment()
result_trees%>% f_meas(truth = Stay,estimate = .pred_class)
```



```{r}
xgb_fit %>% collect_metrics()

result_xgb<-xgb_fit %>% augment()
result_xgb%>% f_meas(truth = Stay,estimate = .pred_class)
```

```{r}
knn_fit%>%collect_metrics()

result_knn<-knn_fit %>% augment()
result_knn%>%f_meas(truth = Stay,estimate = .pred_class)
```


```{r}
svm_fit%>%collect_metrics()
result_svm<-svm_fit %>% augment()
result_svm%>% f_meas(truth = Stay,estimate = .pred_class)

```




```{r}
bag_fit %>% collect_metrics()

result_bag<-bag_fit %>% augment()
result_bag%>% f_meas(truth = Stay,estimate = .pred_class)


```



```{r}
bag_fit %>% collect_metrics()

result_bag<-bag_fit %>% augment()
result_bag%>% f_meas(truth = Stay,estimate = .pred_class)

```

 ####################  
  #######
  ####################

@@ q9

plots of ROC_AUC
WITH VALUES OF AUC

```{r}


plot_roc_curves_single <- function(model_fit, model_name) {
  if (inherits(model_fit, "try-error") || is.null(model_fit)) {
    cat("Skipping", model_name, "- Model fit is invalid or failed.\n")
    return(NULL)
  }
  
  
  preds <- try(model_fit %>% augment(), silent = TRUE)
  if (inherits(preds, "try-error")) {
    cat("Skipping", model_name, "- Failed to augment predictions.\n")
    return(NULL)
  }
  
  preds$Stay <- as.factor(preds$Stay)
  
  classes <- levels(preds$Stay)
  
  roc_list <- list()
  roc_auc_values <- numeric(length(classes))
  
  for (i in seq_along(classes)) {
    class <- classes[i]
    true_binary <- as.numeric(preds$Stay == class)
    prob_col <- paste0(".pred_", class)
    prob <- preds[[prob_col]]
    
    roc_obj <- roc(response = true_binary, predictor = prob, quiet = TRUE)
    roc_list[[class]] <- roc_obj
    roc_auc_values[i] <- auc(roc_obj)
  }
  
  roc_df <- lapply(seq_along(roc_list), function(i) {
    roc <- roc_list[[i]]
    data.frame(
      FPR = 1 - roc$specificities,
      TPR = roc$sensitivities,
      Class = names(roc_list)[i]
    )
  }) %>% bind_rows()
  
  roc_plot <- ggplot(roc_df, aes(x = FPR, y = TPR, color = Class, linetype = Class)) +
    geom_line(size = 1) +
    labs(
      title = paste("Multiclass ROC Curves -", model_name),
      x = "False Positive Rate",
      y = "True Positive Rate"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      legend.title = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5, face = "bold")
    ) +
    scale_color_manual(values = rainbow(length(roc_list))) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray")
  
  cat("\n---", model_name, "---\n")
  cat("ROC-AUC values:\n")
  names(roc_auc_values) <- classes
  print(roc_auc_values)
  
  return(roc_plot)
}

models <- list(
  random_fit = random_fit,
  log_fit = log_fit,
  lgb_fit = lgb_fit,
  tree_fit = tree_fit,
  xgb_fit = xgb_fit,
  knn_fit = knn_fit,
  bag_fit = bag_fit,
  svm_fit=svm_fit
)

results <- lapply(names(models), function(model_name) {
  cat("\nProcessing", model_name, "\n")
  plot <- plot_roc_curves_single(models[[model_name]], model_name)
  if (!is.null(plot)) {
    print(plot)
  }
  return(plot)
})

```

 ####################  
  #######
  ####################

@q10

ranking model BASED ON F1 SCORE 

1 DECISION TREES 
2 BAGGED DECISION TRESS
3 RANDOM FOREST
4 LIGHTGBM
5 XGBOOST
6 ,MULTINOMIAL REGRESSION
7 KNN
8 SVM


Decision Trees (Rank 1): Achieves the highest F1 score, indicating the best balance of precision and recall across all Stay classes. Its simplicity and ability to capture non-linear patterns likely suit the dataset well, especially after SMOTE addressed class imbalance.

Bagged Decision Trees (Rank 2): Performs strongly, slightly below single decision trees. Bagging reduces variance but may not improve much over a well-tuned single tree in this case.

Random Forest (Rank 3): Delivers a robust F1 score, leveraging ensemble averaging and randomization, but is outperformed by simpler tree-based models, possibly due to overfitting or suboptimal tuning.


LightGBM (Rank 4): A gradient boosting model with good performance, but less effective than Random Forest, possibly due to insufficient tuning or sensitivity to the dataset’s characteristics.

XGBoost (Rank 5): Another boosting model, slightly behind LightGBM, suggesting similar tuning or data compatibility issues.

Multinomial Regression (Rank 6): Performs poorly compared to tree-based models, likely because it assumes linear relationships, which may not capture the complex patterns in the data.

KNN (Rank 7): Struggles with a lower F1 score, possibly due to sensitivity to feature scaling, high dimensionality, or class imbalance, despite SMOTE.

SVM (Rank 8): Ranks last, likely due to challenges with multiclass classification, high computational complexity, or poor handling of the imbalanced dataset.



AND HERE WE USED pr_auc vs roc for each class to see the difference between models




```{r}
library(tidymodels)
library(PRROC)
library(pROC)
library(tidyverse)

plot_pr_roc_curves <- function(model_fit, model_name) {
  if (inherits(model_fit, "try-error") || is.null(model_fit)) {
    cat("Skipping", model_name, "- Model fit is invalid or failed.\n")
    return(NULL)
  }
  
  preds <- try(model_fit %>% augment(), silent = TRUE)
  if (inherits(preds, "try-error")) {
    cat("Skipping", model_name, "- Failed to augment predictions.\n")
    return(NULL)
  }
  
  preds$Stay <- as.factor(preds$Stay)
  
  classes <- levels(preds$Stay)
  
  pr_list <- list()
  pr_auc_values <- numeric(length(classes))
  
  for (i in seq_along(classes)) {
    class <- classes[i]
    true_binary <- as.numeric(preds$Stay == class)
    prob_col <- paste0(".pred_", class)
    prob <- preds[[prob_col]]
    
    pr_obj <- pr.curve(
      scores.class0 = prob[true_binary == 1],
      scores.class1 = prob[true_binary == 0],
      curve = TRUE
    )
    
    pr_list[[class]] <- pr_obj
    pr_auc_values[i] <- pr_obj$auc.integral
  }
  
  pr_df <- lapply(seq_along(pr_list), function(i) {
    pr <- pr_list[[i]]
    data.frame(
      Recall = pr$curve[, 1],
      Precision = pr$curve[, 2],
      Class = names(pr_list)[i]
    )
  }) %>% bind_rows()
  
  pr_plot <- ggplot(pr_df, aes(x = Recall, y = Precision)) +
    geom_line(color = "steelblue", size = 1) +
    facet_wrap(~ Class, scales = "free") +
    labs(
      title = paste("Precision-Recall Curve per Class -", model_name),
      x = "Recall",
      y = "Precision"
    ) +
    theme_minimal() +
    geom_abline(slope = -1, intercept = 1, linetype = "dashed", color = "gray")
  
  roc_list <- list()
  roc_auc_values <- numeric(length(classes))
  
  for (i in seq_along(classes)) {
    class <- classes[i]
    true_binary <- as.numeric(preds$Stay == class)
    prob_col <- paste0(".pred_", class)
    prob <- preds[[prob_col]]
    
    roc_obj <- roc(response = true_binary, predictor = prob, quiet = TRUE)
    roc_list[[class]] <- roc_obj
    roc_auc_values[i] <- auc(roc_obj)
  }
  
  roc_df <- lapply(seq_along(roc_list), function(i) {
    roc <- roc_list[[i]]
    data.frame(
      FPR = 1 - roc$specificities,
      TPR = roc$sensitivities,
      Class = names(roc_list)[i]
    )
  }) %>% bind_rows()
  
  roc_plot <- ggplot(roc_df, aes(x = FPR, y = TPR, color = Class, linetype = Class)) +
    geom_line(size = 1) +
    labs(
      title = paste("Multiclass ROC Curves -", model_name),
      x = "False Positive Rate",
      y = "True Positive Rate"
    ) +
    theme_minimal() +
    theme(
      legend.position = "bottom",
      legend.title = element_text(face = "bold"),
      plot.title = element_text(hjust = 0.5, face = "bold")
    ) +
    scale_color_manual(values = rainbow(length(roc_list))) +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray")
  
  cat("\n---", model_name, "---\n")
  cat("PR-AUC values:\n")
  names(pr_auc_values) <- classes
  print(pr_auc_values)
  cat("ROC-AUC values:\n")
  names(roc_auc_values) <- classes
  print(roc_auc_values)
  
  list(pr_plot = pr_plot, roc_plot = roc_plot)
}

models <- list(
  random_fit = random_fit,
  log_fit = log_fit,
  lgb_fit = lgb_fit,
  tree_fit = tree_fit,
  xgb_fit = xgb_fit,
  knn_fit = knn_fit,
  bag_fit = bag_fit,
  svm_fit=svm_fit
)

results <- lapply(names(models), function(model_name) {
  cat("\nProcessing", model_name, "\n")
  plots <- plot_pr_roc_curves(models[[model_name]], model_name)
  if (!is.null(plots)) {
    print(plots$pr_plot)
    print(plots$roc_plot)
  }
  return(plots)
})

```

WE SEE ALSO THAT THE DESICION TREES HEVE THE BEST 
The ROC curves for the decision tree model show strong performance, with most curves trending toward the top-left corner, indicating high TPR at low FPR across classes. This aligns with its top F1 score ranking (Rank 1), suggesting effective class separation, especially for frequent classes. Some curves (likely for rarer classes like '61-70' or '91-100') may dip slightly, reflecting challenges with imbalance despite SMOTE, but overall, the model demonstrates robust multiclass discrimination ability.


 ####################  
  #######
  ####################


q11
 Grid method for each model


```{r}

rand_grid <- grid_random(mtry(range = c(3,13)),
trees(range = c(50,1000)),
min_n(range = c(5,40)),
size = 25)

rand_grid

```




```{r}
library(dials)

param_grid <- parameters(
  cost_complexity(range = c(-5, -1)),  
  min_n(range = c(5, 30)),
  tree_depth(range = c(5, 20))
)

rand_grid <- grid_random(param_grid, size = 5)
```

```{r}
set.seed(2025)
df_cv <- df_train %>% 
  vfold_cv(v = 5, strata = Stay)

```



Random grid (first grid)

```{r}
library(tidymodels)
library(baguette)   
library(foreach)
library(purrr)
library(iterators)
library(parallel)
library(doParallel)
library(tictoc)
library(themis)


ctrl <- control_grid(verbose = TRUE, save_pred = TRUE)

measure <- metric_set(f_meas ,accuracy)
bag_model <- bag_tree(
  cost_complexity = tune(),
  min_n = tune(),
  tree_depth = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

bag_workflow <- workflow() %>%
  add_model(bag_model) %>%
  add_recipe(df_rec1)  

param_grid <- parameters(
  cost_complexity(range = c(-5, -1)),
  min_n(range = c(10, 30)),
  tree_depth(range = c(5, 10))
)

rand_grid <- grid_random(param_grid, size = 5)

bag_tune <- bag_workflow %>%
  tune_grid(
    resamples = df_cv,
    grid = rand_grid,
    metrics = measure,
    control = ctrl
  )





bag_tune %>% collect_metrics()
bag_b<-bag_tune %>% select_best()
bag_f <- finalize_workflow(
bag_workflow,
 bag_b
)
bag_final_fit<-bag_f %>% last_fit(df_split)
bag_final_fit %>% collect_metrics()



result_tun_bag<-bag_final_fit %>% augment()
result_tun_bag%>% f_meas(truth = Stay,estimate = .pred_class)


```



```{r}
random_mod <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
```


regular grid (second grid)


```{r}
set.seed(2025)
df_cv <- df_train %>% vfold_cv(v = 2, strata = Stay)
measure <- metric_set(roc_auc)
ctrl <- control_grid(verbose = TRUE, save_pred = TRUE)
```
```{r}
random_mod <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")


work_random <- workflow() %>%
  add_model(random_mod) %>%
  add_recipe(df_rec1)

reg_grid <- grid_regular(
  mtry(range = c(3, 13)),
  trees(range = c(50, 1000)),
  min_n(range = c(5, 40)),
 
  levels = 2
)

rand_tune_res<-tune_grid(
 work_random,
 resamples = df_cv,
 grid = reg_grid,
 metrics = metric_set(accuracy,recall,specificity),
 control = ctrl
)
rand_tune_res %>% collect_metrics()
rand_b<-rand_tune_res %>% select_best()
final_wf <- finalize_workflow(
work_random,
 rand_b
)
rand_final_fit<-final_wf %>% last_fit(df_split)
rand_final_fit %>% collect_metrics()

rand_tune_res<-rand_final_fit %>% augment()
rand_tune_res%>% f_meas(truth = Stay,estimate = .pred_class)

```

grid_latin_hypercube(3rd grid )



```{r}
set.seed(2025)

tree_mod <- decision_tree(
  tree_depth = tune(),
  min_n = tune(),
  cost_complexity = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

work_tree <- workflow() %>%
  add_model(tree_mod) %>%
  add_recipe(df_rec1)

tree_grid <- grid_regular(
  tree_depth(range = c(5, 15)),
  min_n(range = c(5, 40)),
  cost_complexity(range = c(0.0001, 0.01)),
  levels = 2
)

tree_tune_res <- tune_grid(
  work_tree,
  resamples = df_cv,
  grid = tree_grid,
  metrics = metric_set(accuracy, recall, specificity),
  control = ctrl
)

tree_tune_res %>% collect_metrics()

tree_best <- tree_tune_res %>% select_best()

tree_final_wf <- finalize_workflow(
  work_tree,
  tree_best
)

tree_final_fit <- tree_final_wf %>% last_fit(df_split)

tree_final_fit %>% collect_metrics()

tree_tune_res <- tree_final_fit %>% augment()
tree_tune_res %>%  f_meas(truth = Stay,estimate = .pred_class)
```
The macro-averaged F1 scores for the three models are as follows: Bagged Decision Trees (0.2891478), Random Forest (0.2523182), and Decision Trees (0.1380581). Bagged Decision Trees perform best, achieving the highest F1 score, indicating a better balance of precision and recall across all Stay classes, likely due to the ensemble's ability to reduce variance while benefiting from SMOTE for class imbalance. Random Forest follows, with a moderate F1 score, reflecting decent but less optimal performance compared to Bagged Trees, possibly due to tuning or overfitting. Decision Trees perform the worst, with the lowest F1 score, suggesting poor generalization on the test set, potentially due to overfitting or insufficient handling of rare classes despite the preprocessing steps.



 ####################  
  #######
  ####################
Q12.   yes there is a change Bagged Decision Trees have the best f1 score



 ####################  
  #######
  ####################


Q13 tuning methods.

tune_bayes

```{r}
measure <- metric_set(pr_auc ,accuracy)
ctrl_bayesiane<-control_bayes(verbose = T,verbose_iter = T,no_improve = 50,save_pred = T)
bag_model <- bag_tree(
  cost_complexity = tune(),
  min_n = tune(),
  tree_depth = tune()
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

bag_workflow <- workflow() %>%
  add_model(bag_model) %>%
  add_recipe(df_rec1)  

bag_par<-bag_workflow %>% extract_parameter_set_dials() %>% 
  update( cost_complexity= cost_complexity(range = c(-5, -1)),
 min_n= min_n(range = c(10, 30)),
tree_depth=  tree_depth(range = c(5, 10)))

bag_tune_res<-tune_bayes(bag_workflow,
  resamples=df_cv,
  iter=10,
  param_info=bag_par,
  metrics=metric_set(f_meas) ,
  initial=20,
  control=ctrl_bayesiane
)
bag_tune_res %>% collect_metrics()
bag_best<-bag_tune_res %>% select_best()
bag_final_wf <- finalize_workflow(
  bag_workflow,
  bag_best
)
bG_final_fit<-bag_final_wf %>% last_fit(df_split)
bG_final_fit %>% collect_metrics()

bag_tune_res<-bG_final_fit%>% augment()
bag_tune_res%>%  f_meas(truth = Stay,estimate = .pred_class)




```


```{r}
```


tune_sim_anneal

```{r}
library(finetune)

set.seed(2025)
df_cv <- df_train %>% vfold_cv(v = 3, strata = Stay)
simu_tune<-bag_workflow %>% 
  tune_sim_anneal(resamples = df_cv,
                  iter=20,
                  param_info = bag_par,
                  metrics =metric_set(f_meas),
             
                  control = control_sim_anneal(verbose_iter = T,no_improve = 20,save_pred = T))



simu_tune %>% collect_metrics()
bag_best<-simu_tune %>% select_best()
bag_sim_wf <- finalize_workflow(
  bag_workflow,
  bag_best
)
bG_sim_fit<-bag_sim_wf %>% last_fit(df_split)
bG_sim_fit %>% collect_metrics()

simu_tune<-bG_final_fit%>% augment()
simu_tune%>%  f_meas(truth = Stay,estimate = .pred_class)


```
race_tune
```{r}
set.seed(2025)
library(finetune)

df_cv <- df_train %>% vfold_cv(v = 4, strata = Stay)

race_tune <- bag_workflow %>%
  tune_race_anova(
    resamples = df_cv,
    param_info = bag_par,
    metrics = metric_set(f_meas),
    grid = 10,
    control = control_race(
      verbose_elim = TRUE, 
      save_pred = TRUE,   
    )
  )

race_tune %>% collect_metrics()

bag_best <- race_tune %>% select_best()

bag_race_wf <- finalize_workflow(
  bag_workflow,
  bag_best
)

bag_race_fit <- bag_race_wf %>% last_fit(df_split)

bag_race_fit %>% collect_metrics()

race_predictions <- bag_race_fit %>% augment()

race_predictions %>%
   f_meas(truth = Stay,estimate = .pred_class)
```

race_win_loss_tune
```{r}
set.seed(2025)

df_cv <- df_train %>% vfold_cv(v = 4, strata = Stay)

race_win_loss_tune <- bag_workflow %>%
  tune_race_win_loss(
    resamples = df_cv,
    param_info = bag_par,
    metrics = metric_set(f_meas),
    grid = 20, 
    control = control_race(
      verbose_elim = TRUE,
      save_pred = TRUE,    
      allow_par = TRUE     
    )
  )

race_win_loss_tune %>% collect_metrics()

bag_best <- race_win_loss_tune %>% select_best()

bag_win_loss_wf <- finalize_workflow(
  bag_workflow,
  bag_best
)

bag_win_loss_fit <- bag_win_loss_wf %>% last_fit(df_split)

bag_win_loss_fit %>% collect_metrics()

win_loss_predictions <- bag_win_loss_fit %>% augment()

win_loss_predictions %>%
   f_meas(truth = Stay,estimate = .pred_class)
```



the plots of ROC 


win_loss_predictions


```{r}
bag_win_loss_fit <- win_loss_predictions %>% roc_curve(
                    truth = Stay,`.pred_0-10`,`.pred_11-20`,`.pred_21-30`,`.pred_31-40`,`.pred_41-50`,`.pred_51-60`,`.pred_61-70`
                    ,`.pred_71-80`,`.pred_81-90`,`.pred_91-100`,`.pred_More than 100 Days`,
                    estimator = "macro")
bag_win_loss_fit %>% autoplot()
```
race_predictions

```{r}
bag_race_fit <- race_predictions %>% roc_curve(
                    truth = Stay,`.pred_0-10`,`.pred_11-20`,`.pred_21-30`,`.pred_31-40`,`.pred_41-50`,`.pred_51-60`,`.pred_61-70`
                    ,`.pred_71-80`,`.pred_81-90`,`.pred_91-100`,`.pred_More than 100 Days`,
                    estimator = "macro")
bag_race_fit %>% autoplot()
```
simu_tune


```{r}
bG_sim_fit <- simu_tune %>% roc_curve(
                    truth = Stay,`.pred_0-10`,`.pred_11-20`,`.pred_21-30`,`.pred_31-40`,`.pred_41-50`,`.pred_51-60`,`.pred_61-70`
                    ,`.pred_71-80`,`.pred_81-90`,`.pred_91-100`,`.pred_More than 100 Days`,
                    estimator = "macro")
bG_sim_fit %>% autoplot()
```
bag_tune_res

```{r}

bG_final_fit <- bag_tune_res %>% roc_curve(
                    truth = Stay,`.pred_0-10`,`.pred_11-20`,`.pred_21-30`,`.pred_31-40`,`.pred_41-50`,`.pred_51-60`,`.pred_61-70`
                    ,`.pred_71-80`,`.pred_81-90`,`.pred_91-100`,`.pred_More than 100 Days`,
                    estimator = "macro")
bG_sim_fit %>% autoplot()

```

race_win_loss_tune AND tune_bayes HAVE THE BEST SCORES FOR TUNNINGS






CONCLUSION:

This study evaluated eight machine learning models—Decision Trees, Bagged Decision Trees, Random Forest, LightGBM, XGBoost, Multinomial Regression, KNN, and SVM—to predict hospital length of stay (Stay) across 11 classes, using a dataset of 31,842 cases, 26,289 unique patients, and 32 hospitals. The preprocessing pipeline addressed missing values, normalized numeric predictors, and applied SMOTE to mitigate class imbalance, which was critical given the rarity of certain classes (e.g., '61-70' with as few as 34 instances). Model performance was assessed using macro-averaged F1 scores, PR-AUC, ROC-AUC, and confusion matrices, with tuning methods including grid search, Bayesian optimization, simulated annealing, and racing (ANOVA and win/loss) applied to optimize Bagged Decision Trees.

Decision Trees achieved the highest F1 score (0.289), followed by Bagged Decision Trees (0.252) and Random Forest (0.138), confirming tree-based models’ superiority in handling non-linear patterns and multiclass imbalance. Boosting models (LightGBM, XGBoost) performed moderately, while linear (Multinomial Regression) and distance-based models (KNN, SVM) struggled, likely due to the dataset’s complexity. ROC and PR curves reinforced these findings, with Decision Trees showing the best class separation, though rare classes posed challenges, as evidenced by warnings about undefined precision across tuning methods. Bayesian tuning and simulated annealing for Bagged Decision Trees yielded competitive results, but racing methods highlighted persistent issues with rare class predictions.

In conclusion, Decision Trees and Bagged Decision Trees are recommended for predicting hospital length of stay due to their balanced performance across classes, particularly after SMOTE preprocessing. However, further improvements could involve enhanced handling of rare classes (e.g., via class weights or more aggressive oversampling) and deeper tuning of boosting models to potentially surpass tree-based approaches. These findings underscore the importance of addressing class imbalance in healthcare datasets and leveraging ensemble methods for robust multiclass classification.

```